{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ffmpeg -i zkmultisensor.mp4 -vn -acodec copy zkmultisensor.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing a voice file\n",
    "# import IPython\n",
    "# display(IPython.display.Audio('zkmultisensor/Split_120.mp3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17736596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual downloading any video\n",
    "%pip install yt-dlp\n",
    "%pip install moviepy\n",
    "import yt_dlp\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "def download_youtube_video_split(url, output_path):\n",
    "    ydl_opts = {'outtmpl':output_path,'format_sort': ['res:1080', 'ext:mp4:m4a']}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    \n",
    "    directoryName = output_path.split(\".\")[0]\n",
    "    mp3_file = output_path.split(\".\")[0] + \".mp3\"\n",
    "\n",
    "    videoFile = VideoFileClip(output_path)\n",
    "    audio_clip = videoFile.audio\n",
    "    audio_clip.write_audiofile(mp3_file)\n",
    "\n",
    "    tlen = videoFile.duration \n",
    "    print(tlen)\n",
    "    \n",
    "    os.makedirs(directoryName, exist_ok=True)\n",
    "    for i in range(0, int(tlen), 20):\n",
    "        str_i = str(f'{(i//100)%1000}{(i//10)%10}{i%10}')\n",
    "\n",
    "        viideoPart = videoFile[i:min(tlen,i+20)]\n",
    "        viideoPart.write_videofile(\"{}/Split_{}.mp4\".format(directoryName,str_i))\n",
    "        audioPart = audio_clip[i:min(tlen,i+20)]\n",
    "        audioPart.write_audiofile(\"{}/Split_{}.mp3\".format(directoryName,str_i))\n",
    "        \n",
    "    audio_clip.close()\n",
    "    videoFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List all the TouTube files here\n",
    "url1 = \"https://www.youtube.com/watch?v=VZMBE2NLSC4&t=5s\"\n",
    "download_youtube_video_split(url1, \"zkmultisensor.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory = \"zkmultisensor\"\n",
    "myfilenames = sorted(os.listdir(audio_directory))\n",
    "\n",
    "for filename in myfilenames:\n",
    "    if filename.endswith(\".mp3\"):\n",
    "        filepath = os.path.join(audio_directory, filename)\n",
    "        os.system(f\"ffmpeg -i {filepath} {filepath[:-4]}.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f93ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "audio_directory = \"zkmultisensor\"\n",
    "myfilenames = sorted(os.listdir(audio_directory))\n",
    "print(myfilenames)\n",
    "\n",
    "r = sr.Recognizer()\n",
    "with open('recognized_text.txt', 'w', encoding='utf-8') as output_file:\n",
    "  for chunk_file in myfilenames:\n",
    "    print(chunk_file)\n",
    "    print(chunk_file[-4:])\n",
    "    if chunk_file[-4:] == \".wav\":\n",
    "      print(chunk_file)\n",
    "      filepath = os.path.join(audio_directory, chunk_file)\n",
    "      print(filepath)\n",
    "      with sr.AudioFile(filepath) as source:\n",
    "        audio_data = r.record(source)\n",
    "        try:\n",
    "          text = r.recognize_google(audio_data, language='en-US')  # Persian language code\n",
    "          output_file.write(text + '\\n')\n",
    "        except :\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydocument():\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Page content: {self.page_content}\\nMetadata: {self.metadata}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "mydoc = []\n",
    "with open('recognized_text.txt', 'r', encoding='utf-8') as input_file:\n",
    "    textlines = input_file.readlines()\n",
    "for i, chunk_file in enumerate(filter(lambda x: x.endswith(\".mp4\"), myfilenames)):                      \n",
    "    mydoc.append(Mydocument(textlines[i], {\"source\":chunk_file}))\n",
    "    print(chunk_file)\n",
    "    \n",
    "print(mydoc)\n",
    "\n",
    "############################# Creating Vector database\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "api_key1 = \"aaaaaaa\"\n",
    "embedding1 = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key1)\n",
    "# mydb1 = Chroma.from_documents(totaldoc2, embedding=embedding1)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "alldocs = text_splitter.split_documents(mydoc)\n",
    "print(alldocs)\n",
    "\n",
    "#mydb1 = Chroma.from_documents(alldocs, embedding=embedding1)\n",
    "mydb1 = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embedding1,\n",
    "    persist_directory=\"chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "mydb1.add_documents(alldocs)\n",
    "\n",
    "allok = mydb1.similarity_search(\"What is zkIoT?\", k=1)\n",
    "print(allok)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
