{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12d5499",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Your markdown or comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium webdriver-manager\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import yt_dlp\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, YoutubeLoader, UnstructuredPowerPointLoader \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException, StaleElementReferenceException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb49696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB CRAWLER\n",
    "def crawl_internal_links(start_url, max_pages=50, max_depth=1):\n",
    "    \"\"\"\n",
    "    Crawl internal URLs from a site using Selenium, with support for JavaScript-heavy pages.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL to start crawling from.\n",
    "        max_pages (int): Max number of pages to crawl.\n",
    "        max_depth (int): Max depth to crawl (0 = just root).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of internal URLs that were successfully visited.\n",
    "    \"\"\"\n",
    "    # Selenium Headless Browser Setup\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    visited = set()\n",
    "    domain = urlparse(start_url).netloc\n",
    "    to_visit = [(start_url, 0)]\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url, depth = to_visit.pop(0)\n",
    "        if url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "            time.sleep(1)  # üîÅ Wait for JS to load\n",
    "\n",
    "            print(f\"Visited ({len(visited)+1}/{max_pages}), Depth {depth}): {url}\")\n",
    "            visited.add(url)\n",
    "\n",
    "            # If max depth reached, skip link extraction\n",
    "            if depth == max_depth:\n",
    "                continue\n",
    "\n",
    "            # Extract and queue internal links\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if not href or href.startswith((\"mailto:\", \"tel:\", \"javascript:\")):\n",
    "                        continue\n",
    "\n",
    "                    parsed = urlparse(href)\n",
    "                    if parsed.netloc == domain or parsed.netloc == \"\":\n",
    "                        full_url = urljoin(url, href).split(\"#\")[0]\n",
    "                        if full_url not in visited and all(full_url != q[0] for q in to_visit):\n",
    "                            to_visit.append((full_url, depth + 1))\n",
    "                except StaleElementReferenceException:\n",
    "                    continue\n",
    "\n",
    "        except (WebDriverException, TimeoutException):\n",
    "            print(f\"‚ö†Ô∏è Skipping (Error): {url}\")\n",
    "            visited.add(url)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n Total unique internal URLs visited: {len(visited)}\")\n",
    "\n",
    "    if len(visited) < max_pages:\n",
    "        print(\"‚ö†Ô∏è Number of crawled URLs is less than max_pages. Possible reasons:\")\n",
    "        print(\"- Site may not have enough unique pages within the allowed depth.\")\n",
    "        print(\"- Some links might be hidden behind JavaScript interactions.\")\n",
    "        print(\"- Some links could be blocked, inaccessible, or slow-loading.\")\n",
    "        print(\"- Your max_depth may be too shallow to discover deeper links, try changing depth.\")\n",
    "    \n",
    "    return list(visited)\n",
    "\n",
    "# **************** CONFIGURABLE SETTINGS *********************\n",
    "start_url = \"https://fidesinnova.io/\"\n",
    "max_pages = 53   # Maximum number of pages to visit\n",
    "max_depth = 2    # 0 = only root, 1 = root + links from root\n",
    "\n",
    "#  Run the Crawler\n",
    "web_docs_list1 = crawl_internal_links(start_url, max_pages, max_depth)\n",
    "\n",
    "#  Load the web documents\n",
    "web_docs1 = []\n",
    "for idx in web_docs_list1:\n",
    "    a = WebBaseLoader(idx)\n",
    "    try:\n",
    "        web_docs1.extend(a.load())\n",
    "        print(idx + \" is loaded.\")\n",
    "    except:\n",
    "        print(f\"{idx} is not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1731a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Load environment variables from .env file\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headless setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--log-level=3\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "base_url = \"https://www.fidesinnova.io/\"\n",
    "visited = set()\n",
    "web_docs_list2 = set()\n",
    "\n",
    "def is_valid_internal(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme in [\"http\", \"https\"] and parsed.netloc.endswith(\"fidesinnova.io\")\n",
    "\n",
    "def crawl(url):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Let dynamic content load\n",
    "\n",
    "        links = driver.find_elements(\"tag name\", \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href:\n",
    "                full_url = urljoin(url, href.split(\"#\")[0])  # remove fragments\n",
    "                if is_valid_internal(full_url) and full_url not in web_docs_list2:\n",
    "                    web_docs_list2.add(full_url)\n",
    "                    crawl(full_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error visiting {url}: {e}\")\n",
    "\n",
    "# Start\n",
    "crawl(base_url)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Output\n",
    "print(\"\\n=== Discovered Internal URLs ===\")\n",
    "for url in sorted(web_docs_list2):\n",
    "    print(url)\n",
    "print(f\"\\nTotal internal URLs web_docs_list2: {len(web_docs_list2)}\")\n",
    "\n",
    "\n",
    "#  Load the web documents\n",
    "web_docs2 = []\n",
    "for idx in web_docs_list2:\n",
    "    a = WebBaseLoader(idx)\n",
    "    try:\n",
    "        web_docs2.extend(a.load())\n",
    "        print(idx + \" is loaded.\")\n",
    "    except:\n",
    "        print(f\"{idx} is not loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the token\n",
    "# GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0\",\n",
    "#     \"Accept\": \"application/vnd.github.v3+json\",\n",
    "#     \"Authorization\": f\"token {GITHUB_TOKEN}\"\n",
    "# }\n",
    "\n",
    "# readable_extensions = {\n",
    "#     \".py\", \".json\", \".html\", \".js\", \".ts\", \".css\", \".java\",\n",
    "#     \".c\", \".cpp\", \".rs\", \".txt\", \".md\", \".yml\", \".yaml\", \".sh\"\n",
    "# }\n",
    "\n",
    "# def is_readable_file(filename):\n",
    "#     return any(filename.endswith(ext) for ext in readable_extensions)\n",
    "\n",
    "# def get_default_branch(owner, repo):\n",
    "#     url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "#         if response.status_code == 200:\n",
    "#             return response.json().get(\"default_branch\", \"main\")\n",
    "#         else:\n",
    "#             print(f\"‚ùå Failed to get default branch for {owner}/{repo} ({response.status_code})\")\n",
    "#             return \"main\"\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"‚ùå Exception getting default branch for {owner}/{repo}: {e}\")\n",
    "#         return \"main\"\n",
    "\n",
    "# def list_files_from_api(owner, repo, branch, path=\"\", depth=0, max_depth=10):\n",
    "#     url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"‚ùå Exception accessing {url}: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     if response.status_code == 403:\n",
    "#         print(f\"‚ùå Rate limit hit or access denied for: {url}\")\n",
    "#         return []\n",
    "#     elif response.status_code != 200:\n",
    "#         print(f\"‚ùå Failed to access: {url} ({response.status_code})\")\n",
    "#         return []\n",
    "\n",
    "#     items = response.json()\n",
    "#     files = []\n",
    "\n",
    "#     # Defensive: sometimes API returns a dict on error\n",
    "#     if isinstance(items, dict) and items.get(\"message\"):\n",
    "#         print(f\"‚ùå API error at {url}: {items['message']}\")\n",
    "#         return []\n",
    "\n",
    "#     for item in items:\n",
    "#         if item[\"type\"] == \"file\":\n",
    "#             filename = item[\"name\"]\n",
    "#             if is_readable_file(filename):\n",
    "#                 files.append(item[\"path\"])\n",
    "#         elif item[\"type\"] == \"dir\" and depth < max_depth:\n",
    "#             # Debug print to track recursion progress:\n",
    "#             print(f\"üîç Exploring directory: {item['path']} (depth {depth+1})\")\n",
    "#             files += list_files_from_api(owner, repo, branch, item[\"path\"], depth+1, max_depth)\n",
    "\n",
    "#     return files\n",
    "\n",
    "# github_repos = [\n",
    "#     \"TheArchitect2000/Fides-Innova-WiKi\",\n",
    "#     \"TheArchitect2000/Blockchain-based-IoT-Server\",\n",
    "#     \"TheArchitect2000/ZKP-IoT-Arm-Siemens-IoT2050-C\",\n",
    "#     \"TheArchitect2000/zkiot-riscv-qemu-c\", \n",
    "#     \"TheArchitect2000/Fides-Innova-Smart-Contract-Protocol\",\n",
    "#     \"TheArchitect2000/ZKP-Blockchain-Explorer\",\n",
    "#     \"TheArchitect2000/evm-server\",\n",
    "#     \"TheArchitect2000/New-IoT-Device-Integration\",\n",
    "#     \"TheArchitect2000/zkiot-riscv-qemu-rust\"\n",
    "# ]\n",
    "\n",
    "# github_docs = set()\n",
    "# for full_name in github_repos:\n",
    "#     owner, repo = full_name.split(\"/\")\n",
    "#     print(f\"\\nüì¶ Crawling repository: {full_name}\")\n",
    "#     default_branch = get_default_branch(owner, repo)\n",
    "#     files = list_files_from_api(owner, repo, default_branch)\n",
    "#     if files:\n",
    "#         print(f\"‚úÖ Found {len(files)} readable files:\")\n",
    "#         for f in files:\n",
    "#             t1 = f\"https://github.com/{full_name}/blob/main/{f}\"\n",
    "#             print(\"   -\", t1)\n",
    "#             github_docs.add(t1)\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è No readable files found or repo is empty/private.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GitHub Repos\n",
    "github_repos = [\n",
    "    \"https://github.com/TheArchitect2000/Fides-Innova-WiKi\",\n",
    "    \"https://github.com/TheArchitect2000/Blockchain-based-IoT-Server\",\n",
    "    \"https://github.com/TheArchitect2000/ZKP-IoT-Arm-Siemens-IoT2050-C\",\n",
    "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-c\", \n",
    "    \"https://github.com/TheArchitect2000/Fides-Innova-Smart-Contract-Protocol\",\n",
    "    \"https://github.com/TheArchitect2000/ZKP-Blockchain-Explorer\",\n",
    "    \"https://github.com/TheArchitect2000/evm-server\",\n",
    "    \"https://github.com/TheArchitect2000/New-IoT-Device-Integration\",\n",
    "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-rust\"\n",
    "]\n",
    "\n",
    "github_docs = []\n",
    "\n",
    "for url in github_repos:\n",
    "    print(f\"üì• Loading repository: {url}\")\n",
    "    repo_name = url.split(\"/\")[-1]\n",
    "    local_path = f\"./cloned_repos/{repo_name}\"\n",
    "\n",
    "    loader = GitLoader(\n",
    "        repo_path=local_path,\n",
    "        clone_url=url,\n",
    "        branch=\"main\",\n",
    "        file_filter=lambda f: f.endswith((\n",
    "            \".py\", \".md\", \".c\", \".cpp\", \".rs\", \".json\", \".html\",\n",
    "            \".js\", \".ts\", \".css\", \".java\", \".txt\", \".yml\", \".yaml\", \".sh\"\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    github_docs = loader.load()\n",
    "    print(f\"‚úÖ Loaded {len(github_docs)} documents from {repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef84b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: YouTube Channel Crawler ---\n",
    "\n",
    "# Fides Innova YouTube Channel ID\n",
    "CHANNEL_ID = \"UCrrqGYx98H1dPdZsNb1i9-g\"\n",
    "CHANNEL_URL = f\"https://www.youtube.com/channel/{CHANNEL_ID}\"\n",
    "\n",
    "def fetch_video_urls(channel_url: str):\n",
    "    ydl_opts = {\n",
    "        'ignoreerrors': True,\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,  # Only metadata, not downloading\n",
    "        'force_generic_extractor': False,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(channel_url, download=False)\n",
    "        video_urls = []\n",
    "        video_ids = []\n",
    "\n",
    "        if 'entries' in result:\n",
    "            for entry in result['entries']:\n",
    "                if entry and 'id' in entry:\n",
    "                    video_url = f\"https://www.youtube.com/watch?v={entry['id']}\"\n",
    "                    video_urls.append(video_url)\n",
    "                    video_ids.append(entry['id'])\n",
    "        return video_ids\n",
    "\n",
    "# Fetch and display video URLs\n",
    "video_ids = fetch_video_urls(CHANNEL_URL)\n",
    "print(f\"‚úÖ Found {len(video_ids)} videos on channel.\\n\")\n",
    "\n",
    "youtube_docs = []\n",
    "second_try_idx = []\n",
    "third_try_idx = []\n",
    "for idx in video_ids:\n",
    "    a = YoutubeLoader(idx)\n",
    "    try:\n",
    "        youtube_docs.extend(a.load())\n",
    "        print(idx + \" is loaded.\")\n",
    "    except:\n",
    "        print(f\"{idx} is not loaded.\")\n",
    "        second_try_idx.append(str(idx))\n",
    "        print(\"\\n unloaded list \")\n",
    "        print(second_try_idx)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"second try for unloded videos.\")\n",
    "\n",
    "for idx in second_try_idx:\n",
    "    print(\"waiting 10 seconds...\")\n",
    "    time.sleep(10)\n",
    "    a = YoutubeLoader(idx)\n",
    "    try:\n",
    "        youtube_docs.extend(a.load())\n",
    "        print(idx + \" is loaded in the second try.\")\n",
    "    except:\n",
    "        print(\"\\n\")\n",
    "        print(f\"{idx} is not loaded in the second try.\")\n",
    "        third_try_idx.append(str(idx))\n",
    "        print(\"\\n unloaded list for the second time \")\n",
    "        print(second_try_idx)\n",
    "        print(\"\\n\")\n",
    "\n",
    "def change_YouTube_doc(doc):\n",
    "    doc.metadata['type']='YouTube'\n",
    "    return doc\n",
    "\n",
    "youtube_docs = list(map(change_YouTube_doc, youtube_docs))\n",
    "\n",
    "print(\"=== ALL youtube videos ===)\")\n",
    "print(len(youtube_docs))\n",
    "print(\"=== Loaded youtube videos ===)\")\n",
    "print(len(youtube_docs)-len(third_try_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59179ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Load PDFs ---\n",
    "pdf_docs = []\n",
    "pdf_files = [\n",
    "    \"PDF/zkIoT.pdf\",\n",
    "]\n",
    "\n",
    "for path in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pdf_docs.extend(loader.load())\n",
    "        print(len(pdf_docs))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF {path}: {e}\")\n",
    "\n",
    "def change_pdf_doc(doc):\n",
    "    doc.metadata['type']='PDF'\n",
    "    return doc\n",
    "\n",
    "pdf_docs = list(map(change_pdf_doc, pdf_docs))\n",
    "\n",
    "print(\"=== ALL PDF docs ===)\")\n",
    "print(len(pdf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pptx_docs = []\n",
    "pptx_files = [\n",
    "    \"PPTX/FidesinnovaDeck-v11.pptx\"\n",
    "]\n",
    "\n",
    "for path in pptx_files:\n",
    "    try:\n",
    "        loader = UnstructuredPowerPointLoader(path)\n",
    "        pptx_docs.extend(loader.load())\n",
    "        print(f\"‚úÖ Loaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PPTX {path}: {e}\")\n",
    "\n",
    "# Add metadata\n",
    "def change_pptx_doc(doc):\n",
    "    doc.metadata['type'] = 'PPTX'\n",
    "    return doc\n",
    "\n",
    "pptx_docs = list(map(change_pptx_doc, pptx_docs))\n",
    "\n",
    "print(\"=== ALL PPTX docs ===)\")\n",
    "print(len(pptx_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Split & Vectorize ---\n",
    "# all_docs =  youtube_docs + pdf_docs + pptx_docs + web_docs1 + web_docs2 + github_docs\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "split_docs1 = splitter.split_documents(youtube_docs)\n",
    "split_docs2 = splitter.split_documents(pdf_docs)\n",
    "split_docs3 = splitter.split_documents(pptx_docs)\n",
    "split_docs4 = splitter.split_documents(web_docs1)\n",
    "split_docs5 = splitter.split_documents(web_docs2)\n",
    "split_docs6 = splitter.split_documents(github_docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"fides_crawled_data\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "\n",
    "vectordb.add_documents(split_docs1)\n",
    "vectordb.add_documents(split_docs2)\n",
    "vectordb.add_documents(split_docs3)\n",
    "vectordb.add_documents(split_docs4)\n",
    "vectordb.add_documents(split_docs5)\n",
    "\n",
    "# Instead of adding GitHun large documents using:\n",
    "# vectordb.add_documents(split_docs6)\n",
    "# we do this:\n",
    "batch_size = 100 \n",
    "for i in range(0, len(split_docs6), batch_size):\n",
    "    batch = split_docs6[i:i + batch_size]\n",
    "    vectordb.add_documents(batch)\n",
    "    print(f\"‚úÖ Added batch {i // batch_size + 1} of {len(split_docs6)/batch_size} documents\")\n",
    "\n",
    "print(\"‚úÖ All documents crawled, split, and stored in vector DB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb537074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of adding GitHun large documents using:\n",
    "# vectordb.add_documents(split_docs6)\n",
    "# we do this:\n",
    "batch_size = 100 \n",
    "for i in range(0, len(split_docs6), batch_size):\n",
    "    batch = split_docs6[i:i + batch_size]\n",
    "    vectordb.add_documents(batch)\n",
    "    print(f\"‚úÖ Added batch {i // batch_size + 1} of {len(split_docs6)/batch_size} documents\")\n",
    "\n",
    "print(\"‚úÖ All documents crawled, split, and stored in vector DB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f70d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectordb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
