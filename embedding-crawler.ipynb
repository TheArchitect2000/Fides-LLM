{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wixnYrF5C1tR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for embedding.py file\n",
        "# python3 -m venv venv\n",
        "# source venv/bin/activate\n",
        "# pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 SpeechRecognition opencv-python\n",
        "# python3 embedding.py\n",
        "# pip install pyaudio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pip -q\n",
        "%pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 -q\n",
        "%pip install youtube-transcript-api bs4 pypdf -q\n",
        "%pip install SpeechRecognition -q \n",
        "%pip install opencv-python -q\n",
        "%pip install beautifulsoup4 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, YoutubeLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Web crawler\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; FidesCrawler/1.0)\"\n",
        "}\n",
        "\n",
        "web_urls = [\n",
        "    \"https://www.fidesinnova.io/\",\n",
        "    \"https://fidesinnova.io/devices/\",\n",
        "    \"https://fidesinnova.io/Contacts/\",\n",
        "    \"https://fidesinnova.io/courses/\",\n",
        "    \"https://linktr.ee/fidesinnova/\",\n",
        "    \"https://explorer.fidesinnova.io/\",\n",
        "    \"https://play.google.com/store/apps/details?id=io.fidesinnova.front&pli=1/\",\n",
        "    \"https://apps.apple.com/ca/app/fidesinnova/id6477492757/\",\n",
        "    \"https://fidesinnova.io/blog-standard/\",\n",
        "    \"https://fidesinnova.io/About/\",\n",
        "    \"https://fidesinnova.io/#articles/\",\n",
        "    \"https://discord.com/invite/NQdM6JGwcs/\",\n",
        "    \"https://www.youtube.com/@FidesInnova/playlists/\",\n",
        "    \"https://x.com/FidesInnova/\",\n",
        "    \"https://fidesinnova.io/service-market/\",\n",
        "    \"https://fidesinnova.io/service-contract-2/\",\n",
        "    \"https://fidesinnova.io/d2pos/\",\n",
        "    \"https://fidesinnova.io/web3/\",\n",
        "    \"https://fidesinnova.io/service-contract/\",\n",
        "    \"https://fidesinnova.io/miotn/\",\n",
        "    \"https://fidesinnova.io/consensus-algorithms/\",\n",
        "    \"https://fidesinnova.io/mqtt-mqtts/\"\n",
        "]\n",
        "\n",
        "def crawl_web_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"Web\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "web_docs = [doc for url in web_urls if (doc := crawl_web_url(url))]\n",
        "web_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_github_fides_wiki():\n",
        "    url = \"https://github.com/TheArchitect2000/Fides-Innova-WiKi/\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n",
        "                      \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"Web\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling GitHub: {e}\")\n",
        "        return None\n",
        "\n",
        "github_doc = crawl_github_fides_wiki()\n",
        "github_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: GitHub Crawler ---\n",
        "github_urls = [\n",
        "    \"https://github.com/TheArchitect2000/iot-server/\",\n",
        "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-c/\",\n",
        "    \"https://github.com/TheArchitect2000/Fides-Innova-WiKi/\",\n",
        "    \"https://github.com/TheArchitect2000/zkiot-arm-siemens-iot2050-c/\",\n",
        "    \"https://github.com/TheArchitect2000/fidesinnova-smart-contract-protocol/\",\n",
        "    \"https://github.com/TheArchitect2000/zkp-explorer/\",\n",
        "    \"https://github.com/TheArchitect2000/Fides-LLM/\",\n",
        "    \"https://github.com/TheArchitect2000/Verifiable-Computing-AI-Agents/\",\n",
        "    \"https://github.com/TheArchitect2000/evm-server/\",\n",
        "    \"https://github.com/TheArchitect2000/New-IoT-Device-Integration/\",\n",
        "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-rust/\"\n",
        "]\n",
        "\n",
        "def crawl_github_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"Web\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling GitHub {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "github_docs = [doc for url in github_urls if (doc := crawl_github_url(url))]\n",
        "github_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 5: YouTube Channel Crawler ---\n",
        "import yt_dlp\n",
        "\n",
        "# Fides Innova YouTube Channel ID\n",
        "CHANNEL_ID = \"UCrrqGYx98H1dPdZsNb1i9-g\"\n",
        "CHANNEL_URL = f\"https://www.youtube.com/channel/{CHANNEL_ID}\"\n",
        "\n",
        "def fetch_video_urls(channel_url: str):\n",
        "    ydl_opts = {\n",
        "        'ignoreerrors': True,\n",
        "        'quiet': True,\n",
        "        'extract_flat': True,  # Only metadata, not downloading\n",
        "        'force_generic_extractor': False,\n",
        "    }\n",
        "    \n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        result = ydl.extract_info(channel_url, download=False)\n",
        "        video_urls = []\n",
        "        video_ids = []\n",
        "        \n",
        "        if 'entries' in result:\n",
        "            for entry in result['entries']:\n",
        "                if entry and 'id' in entry:\n",
        "                    video_url = f\"https://www.youtube.com/watch?v={entry['id']}\"\n",
        "                    video_urls.append(video_url)\n",
        "                    video_ids.append(entry['id'])                    \n",
        "        return video_ids\n",
        "\n",
        "# Fetch and display video URLs\n",
        "video_ids = fetch_video_urls(CHANNEL_URL)\n",
        "print(f\"âœ… Found {len(video_ids)} videos on channel.\\n\")\n",
        "\n",
        "youtube_docs = []\n",
        "for idx in video_ids:\n",
        "    a = YoutubeLoader(idx)\n",
        "    try:\n",
        "        youtube_docs.extend(a.load())\n",
        "        print(idx)\n",
        "    except:\n",
        "        print(f\"{idx} not loaded.\")\n",
        "        try:\n",
        "            time.sleep(4)\n",
        "            youtube_docs.extend(a.load())\n",
        "            print(idx)\n",
        "        except:\n",
        "            print(f\"{idx} not loaded.\")\n",
        "            pass\n",
        "\n",
        "def change_YouTube_doc(doc):\n",
        "    doc.metadata['type']='YouTube'\n",
        "    return doc\n",
        "\n",
        "youtube_docs = list(map(change_YouTube_doc, youtube_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2025.04.30'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import yt_dlp\n",
        "import yt_dlp.version\n",
        "\n",
        "yt_dlp.version.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "youtube_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 6: Load PDFs ---\n",
        "pdf_docs = []\n",
        "pdf_files = [\n",
        "    \"PDF/zkIoT.pdf\",\n",
        "    \"PDF/Fides Innova Company Profile 2025.pdf\",\n",
        "    \"PDF/Fides Innova Pitch Deck - v10.pdf\"\n",
        "]\n",
        "\n",
        "for path in pdf_files:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        pdf_docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF {path}: {e}\")\n",
        "\n",
        "def change_pdf_doc(doc):\n",
        "    doc.metadata['type']='PDF'\n",
        "    return doc\n",
        "\n",
        "pdf_docs = list(map(change_pdf_doc, pdf_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 7: Split & Vectorize ---\n",
        "all_docs = web_docs + github_docs + youtube_docs + pdf_docs\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_docs = splitter.split_documents(all_docs)\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "vectordb = Chroma(\n",
        "    collection_name=\"fides_crawled_data\",\n",
        "    embedding_function=embedding,\n",
        "    persist_directory=\"./chroma_langchain_db\"\n",
        ")\n",
        "vectordb.add_documents(split_docs)\n",
        "print(\"âœ… All documents crawled, split, and stored in vector DB.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_docs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
