{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wixnYrF5C1tR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for embedding.py fiel\n",
        "# python3 -m venv venv\n",
        "# source venv/bin/activate\n",
        "# pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 SpeechRecognition opencv-python\n",
        "# python3 embedding.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1694.47s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1700.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1707.02s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1712.98s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1718.92s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1724.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/drmanu/Library/Python/3.9/lib/python/site-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/drmanu/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/drmanu/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (4.13.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#%pip install pyaudio -q\n",
        "%pip install --upgrade pip -q\n",
        "%pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 -q\n",
        "%pip install youtube-transcript-api bs4 pypdf -q\n",
        "%pip install SpeechRecognition -q \n",
        "%pip install opencv-python -q\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: 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\n",
            "SERPER_API_KEY: dummy\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(dotenv_path=\"/Users/drmanu/Desktop/my_internship/codebase/Fides-LLM/.env\")\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
        "if not serper_api_key:\n",
        "    raise EnvironmentError(\"SERPER_API_KEY is not set.\")\n",
        "\n",
        "print(f\"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}\")\n",
        "print(f\"SERPER_API_KEY: {os.getenv('SERPER_API_KEY')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error crawling https://x.com/FidesInnova/: 400 Client Error: Bad Request for url: https://x.com/FidesInnova/\n"
          ]
        }
      ],
      "source": [
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; FidesCrawler/1.0)\"\n",
        "}\n",
        "\n",
        "web_urls = [\n",
        "    \"https://www.fidesinnova.io/\",\n",
        "    \"https://fidesinnova.io/devices/\",\n",
        "    \"https://fidesinnova.io/Contacts/\",\n",
        "    \"https://fidesinnova.io/courses/\",\n",
        "    \"https://linktr.ee/fidesinnova/\",\n",
        "    \"https://explorer.fidesinnova.io/\",\n",
        "    \"https://play.google.com/store/apps/details?id=io.fidesinnova.front&pli=1/\",\n",
        "    \"https://apps.apple.com/ca/app/fidesinnova/id6477492757/\",\n",
        "    \"https://fidesinnova.io/blog-standard/\",\n",
        "    \"https://fidesinnova.io/About/\",\n",
        "    \"https://fidesinnova.io/#articles/\",\n",
        "    \"https://discord.com/invite/NQdM6JGwcs/\",\n",
        "    \"https://www.youtube.com/@FidesInnova/playlists/\",\n",
        "    \"https://x.com/FidesInnova/\",\n",
        "    \"https://fidesinnova.io/service-market/\",\n",
        "    \"https://fidesinnova.io/service-contract-2/\",\n",
        "    \"https://fidesinnova.io/d2pos/\",\n",
        "    \"https://fidesinnova.io/web3/\",\n",
        "    \"https://fidesinnova.io/service-contract/\",\n",
        "    \"https://fidesinnova.io/miotn/\",\n",
        "    \"https://fidesinnova.io/consensus-algorithms/\",\n",
        "    \"https://fidesinnova.io/mqtt-mqtts/\"\n",
        "]\n",
        "\n",
        "def crawl_web_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"Web\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "web_docs = [doc for url in web_urls if (doc := crawl_web_url(url))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_github_fides_wiki():\n",
        "    url = \"https://github.com/TheArchitect2000/Fides-Innova-WiKi/\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n",
        "                      \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"GitHub\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling GitHub: {e}\")\n",
        "        return None\n",
        "\n",
        "github_doc = crawl_github_fides_wiki()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: GitHub Crawler ---\n",
        "github_urls = [\n",
        "    \"https://github.com/TheArchitect2000/iot-server/\",\n",
        "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-c/\",\n",
        "    \"https://github.com/TheArchitect2000/Fides-Innova-WiKi/\",\n",
        "    \"https://github.com/TheArchitect2000/zkiot-arm-siemens-iot2050-c.git/\"\n",
        "]\n",
        "\n",
        "def crawl_github_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"GitHub\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling GitHub {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "github_docs = [doc for url in github_urls if (doc := crawl_github_url(url))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 5: YouTube Channel Crawler ---\n",
        "youtube_url = \"https://www.youtube.com/@Fidesinnova/videos\"\n",
        "\n",
        "def crawl_youtube_channel(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "        description = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "        desc_text = description['content'] if description else \"No description available.\"\n",
        "        return Document(\n",
        "            page_content=f\"Channel: FidesInnova\\nURL: {url}\\n\\n{desc_text}\",\n",
        "            metadata={\"source\": url, \"title\": title, \"type\": \"YouTube\"}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling YouTube: {e}\")\n",
        "        return None\n",
        "\n",
        "youtube_doc = crawl_youtube_channel(youtube_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 6: Load PDFs ---\n",
        "pdf_docs = []\n",
        "pdf_files = [\n",
        "    \"PDF/zkIoT.pdf\",\n",
        "    \"PDF/Fides Innova Company Profile 2025.pdf\",\n",
        "    \"PDF/Fides Innova Pitch Deck - v10.pdf\"\n",
        "]\n",
        "\n",
        "for path in pdf_files:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        pdf_docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF {path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/pr/9jz5fmk91wsd647d487l76680000gn/T/ipykernel_63807/2703807842.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectordb = Chroma(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All documents crawled, split, and stored in vector DB.\n"
          ]
        }
      ],
      "source": [
        "# --- Step 7: Split & Vectorize ---\n",
        "all_docs = web_docs + github_docs + ([youtube_doc] if youtube_doc else []) + pdf_docs\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_docs = splitter.split_documents(all_docs)\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "vectordb = Chroma(\n",
        "    collection_name=\"fides_crawled_data\",\n",
        "    embedding_function=embedding,\n",
        "    persist_directory=\"chroma_langchain_db\"\n",
        ")\n",
        "\n",
        "vectordb.add_documents(split_docs)\n",
        "print(\"✅ All documents crawled, split, and stored in vector DB.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
