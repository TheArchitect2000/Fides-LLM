{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12d5499",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Your markdown or comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb49696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEB CRAWLER\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException, StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def crawl_internal_links(start_url, max_pages=50, max_depth=1):\n",
    "    \"\"\"\n",
    "    Crawl internal URLs from a site using Selenium, with support for JavaScript-heavy pages.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL to start crawling from.\n",
    "        max_pages (int): Max number of pages to crawl.\n",
    "        max_depth (int): Max depth to crawl (0 = just root).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of internal URLs that were successfully visited.\n",
    "    \"\"\"\n",
    "    # Selenium Headless Browser Setup\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    visited = set()\n",
    "    domain = urlparse(start_url).netloc\n",
    "    to_visit = [(start_url, 0)]\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url, depth = to_visit.pop(0)\n",
    "        if url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "            time.sleep(1)  # üîÅ Wait for JS to load\n",
    "\n",
    "            print(f\"Visiting ({len(visited)+1}/{max_pages}), Depth {depth}): {url}\")\n",
    "            visited.add(url)\n",
    "\n",
    "            # If max depth reached, skip link extraction\n",
    "            if depth == max_depth:\n",
    "                continue\n",
    "\n",
    "            # Extract and queue internal links\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if not href or href.startswith((\"mailto:\", \"tel:\", \"javascript:\")):\n",
    "                        continue\n",
    "\n",
    "                    parsed = urlparse(href)\n",
    "                    if parsed.netloc == domain or parsed.netloc == \"\":\n",
    "                        full_url = urljoin(url, href).split(\"#\")[0]\n",
    "                        if full_url not in visited and all(full_url != q[0] for q in to_visit):\n",
    "                            to_visit.append((full_url, depth + 1))\n",
    "                except StaleElementReferenceException:\n",
    "                    continue\n",
    "\n",
    "        except (WebDriverException, TimeoutException):\n",
    "            print(f\"‚ö†Ô∏è Skipping (Error): {url}\")\n",
    "            visited.add(url)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n Total unique internal URLs visited: {len(visited)}\")\n",
    "\n",
    "    if len(visited) < max_pages:\n",
    "        print(\"‚ö†Ô∏è Number of crawled URLs is less than max_pages. Possible reasons:\")\n",
    "        print(\"- Site may not have enough unique pages within the allowed depth.\")\n",
    "        print(\"- Some links might be hidden behind JavaScript interactions.\")\n",
    "        print(\"- Some links could be blocked, inaccessible, or slow-loading.\")\n",
    "        print(\"- Your max_depth may be too shallow to discover deeper links, try changing depth.\")\n",
    "    \n",
    "    return list(visited)\n",
    "\n",
    "# ****************                CONFIGURABLE SETTINGS                *********************\n",
    "\n",
    "start_url = \"https://fidesinnova.io/\"\n",
    "max_pages = 53   # Maximum number of pages to visit\n",
    "max_depth = 2    # 0 = only root, 1 = root + links from root\n",
    "\n",
    "\n",
    "#  Run the Crawler\n",
    "\n",
    "urls = crawl_internal_links(start_url, max_pages, max_depth)\n",
    "\n",
    "# Optionally: Use the `urls` list for further processing (not printed again here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 1: \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import yt_dlp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, YoutubeLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1731a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Load environment variables from .env file\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "# Headless setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--log-level=3\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "base_url = \"https://www.fidesinnova.io/\"\n",
    "visited = set()\n",
    "discovered = set()\n",
    "\n",
    "def is_valid_internal(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme in [\"http\", \"https\"] and parsed.netloc.endswith(\"fidesinnova.io\")\n",
    "\n",
    "def crawl(url):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Let dynamic content load\n",
    "\n",
    "        links = driver.find_elements(\"tag name\", \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href:\n",
    "                full_url = urljoin(url, href.split(\"#\")[0])  # remove fragments\n",
    "                if is_valid_internal(full_url) and full_url not in discovered:\n",
    "                    discovered.add(full_url)\n",
    "                    crawl(full_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error visiting {url}: {e}\")\n",
    "\n",
    "# Start\n",
    "crawl(base_url)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Output\n",
    "print(\"\\n=== Discovered Internal URLs ===\")\n",
    "for url in sorted(discovered):\n",
    "    print(url)\n",
    "print(f\"\\nTotal internal URLs discovered: {len(discovered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: GitHub Crawler ---\n",
    "github_urls = [\n",
    "    \"https://github.com/TheArchitect2000/Fides-Innova-WiKi/\",\n",
    "    \"https://github.com/TheArchitect2000/Blockchain-based-IoT-Server\",\n",
    "    \"https://github.com/TheArchitect2000/ZKP-IoT-Arm-Siemens-IoT2050-C\",\n",
    "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-c\",\n",
    "    \"https://github.com/TheArchitect2000/Fides-Innova-Smart-Contract-Protocol\",\n",
    "    \"https://github.com/TheArchitect2000/ZKP-Blockchain-Explorer\",\n",
    "    \"https://github.com/TheArchitect2000/evm-server\",\n",
    "    \"https://github.com/TheArchitect2000/New-IoT-Device-Integration\",\n",
    "    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-rust\"    \n",
    "]\n",
    "\n",
    "def crawl_github_url(url):\n",
    "    \n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n",
    "                    \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string.strip() if soup.title else \"No Title\"\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return Document(page_content=text, metadata={\"source\": url, \"title\": title, \"type\": \"Web\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling GitHub {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "github_docs = [doc for url in github_urls if (doc := crawl_github_url(url))]\n",
    "\n",
    "print(\"=== ALL github_docs ===)\")\n",
    "print(len(github_docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef84b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: YouTube Channel Crawler ---\n",
    "\n",
    "# Fides Innova YouTube Channel ID\n",
    "CHANNEL_ID = \"UCrrqGYx98H1dPdZsNb1i9-g\"\n",
    "CHANNEL_URL = f\"https://www.youtube.com/channel/{CHANNEL_ID}\"\n",
    "\n",
    "def fetch_video_urls(channel_url: str):\n",
    "    ydl_opts = {\n",
    "        'ignoreerrors': True,\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,  # Only metadata, not downloading\n",
    "        'force_generic_extractor': False,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(channel_url, download=False)\n",
    "        video_urls = []\n",
    "        video_ids = []\n",
    "\n",
    "        if 'entries' in result:\n",
    "            for entry in result['entries']:\n",
    "                if entry and 'id' in entry:\n",
    "                    video_url = f\"https://www.youtube.com/watch?v={entry['id']}\"\n",
    "                    video_urls.append(video_url)\n",
    "                    video_ids.append(entry['id'])\n",
    "        return video_ids\n",
    "\n",
    "# Fetch and display video URLs\n",
    "video_ids = fetch_video_urls(CHANNEL_URL)\n",
    "print(f\"‚úÖ Found {len(video_ids)} videos on channel.\\n\")\n",
    "\n",
    "youtube_docs = []\n",
    "second_try_idx = []\n",
    "third_try_idx = []\n",
    "for idx in video_ids:\n",
    "    a = YoutubeLoader(idx)\n",
    "    try:\n",
    "        youtube_docs.extend(a.load())\n",
    "        print(idx + \" is loaded.\")\n",
    "    except:\n",
    "        print(f\"{idx} is not loaded.\")\n",
    "        second_try_idx.append(str(idx))\n",
    "        print(\"\\n unloaded list \")\n",
    "        print(second_try_idx)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"second try for unloded videos.\")\n",
    "\n",
    "for idx in second_try_idx:\n",
    "    print(\"waiting 10 seconds...\")\n",
    "    time.sleep(10)\n",
    "    a = YoutubeLoader(idx)\n",
    "    try:\n",
    "        youtube_docs.extend(a.load())\n",
    "        print(idx + \" is loaded in the second try.\")\n",
    "    except:\n",
    "        print(\"\\n\")\n",
    "        print(f\"{idx} is not loaded in the second try.\")\n",
    "        third_try_idx.append(str(idx))\n",
    "        print(\"\\n unloaded list for the second time \")\n",
    "        print(second_try_idx)\n",
    "        print(\"\\n\")\n",
    "\n",
    "def change_YouTube_doc(doc):\n",
    "    doc.metadata['type']='YouTube'\n",
    "    return doc\n",
    "\n",
    "youtube_docs = list(map(change_YouTube_doc, youtube_docs))\n",
    "\n",
    "print(\"=== ALL youtube videos ===)\")\n",
    "print(len(youtube_docs))\n",
    "print(\"=== Loaded youtube videos ===)\")\n",
    "print(len(youtube_docs)-len(third_try_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59179ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Load PDFs ---\n",
    "pdf_docs = []\n",
    "pdf_files = [\n",
    "    \"PDF/zkIoT.pdf\",\n",
    "]\n",
    "\n",
    "for path in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pdf_docs.extend(loader.load())\n",
    "        print(len(pdf_docs))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF {path}: {e}\")\n",
    "\n",
    "def change_pdf_doc(doc):\n",
    "    doc.metadata['type']='PDF'\n",
    "    return doc\n",
    "\n",
    "pdf_docs = list(map(change_pdf_doc, pdf_docs))\n",
    "\n",
    "print(\"=== ALL PDF docs ===)\")\n",
    "print(len(pdf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pptx_docs = []\n",
    "pptx_files = [\n",
    "    \"PPTX/FidesinnovaDeck-v11.pptx\"\n",
    "]\n",
    "\n",
    "for path in pptx_files:\n",
    "    try:\n",
    "        loader = UnstructuredPowerPointLoader(path)\n",
    "        pptx_docs.extend(loader.load())\n",
    "        print(f\"‚úÖ Loaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PPTX {path}: {e}\")\n",
    "\n",
    "# Add metadata\n",
    "def change_pptx_doc(doc):\n",
    "    doc.metadata['type'] = 'PPTX'\n",
    "    return doc\n",
    "\n",
    "pptx_docs = list(map(change_pptx_doc, pptx_docs))\n",
    "\n",
    "print(\"=== ALL PPTX docs ===)\")\n",
    "print(len(pptx_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Split & Vectorize ---\n",
    "all_docs = web_docs + github_docs + youtube_docs + pdf_docs + pptx_docs\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "split_docs = splitter.split_documents(all_docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"fides_crawled_data\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./chroma_langchain_db2\"\n",
    ")\n",
    "\n",
    "vectordb.add_documents(split_docs)\n",
    "# print(\"‚úÖ All documents crawled, split, and stored in vector DB.\")\n",
    "\n",
    "# print(\"=== ALL Doc.s ===)\")\n",
    "# print(len(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f70d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectordb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
