{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python3.10 -m venv venv\n",
    "!source venv/bin/activate\n",
    "!python3.10 -m pip install git+https://github.com/openai/whisper.git\n",
    "!python3.10 -m pip install pypdf bs4 yt-dlp langchain faiss-cpu whisper scikit-learn regex selenium \n",
    "!python3.10 -m pip install python-pptx webdriver_manager GitPython unstructured langchain_openai langchain-community arxiv wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11cd0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-airflow==3.0.2 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-3.0.2/constraints-3.10.txt\n"
     ]
    }
   ],
   "source": [
    "!export AIRFLOW_HOME=~/airflow\n",
    "AIRFLOW_VERSION=\"3.0.2\"\n",
    "# !PYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\n",
    "PYTHON_VERSION=\"3.10\"\n",
    "CONSTRAINT_URL=f\"https://raw.githubusercontent.com/apache/airflow/constraints-{AIRFLOW_VERSION}/constraints-{PYTHON_VERSION}.txt\"\n",
    "# For example this would install 3.0.0 with python 3.9: https://raw.githubusercontent.com/apache/airflow/constraints-3.0.2/constraints-3.9.txt\n",
    "arg = f\"apache-airflow=={AIRFLOW_VERSION} --constraint {CONSTRAINT_URL}\"\n",
    "print(arg)\n",
    "!pip install {arg} -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f890a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "def setup_environment():\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    # --- Step 2: Load environment variables from .env file\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"Environment setup complete.\")\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490f9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler_youtube.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile crawler_youtube.py\n",
    "\n",
    "def fetch_video():\n",
    "    ydl_opts = {\n",
    "        'ignoreerrors': True,\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,\n",
    "        'force_generic_extractor': False,\n",
    "    }\n",
    "    # Channel Details\n",
    "    CHANNEL_ID = \"UCrrqGYx98H1dPdZsNb1i9-g\"\n",
    "    CHANNEL_URL = f\"https://www.youtube.com/channel/{CHANNEL_ID}\"\n",
    "    print(\"üé¨ YouTube Crawler started ...\")\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(CHANNEL_URL, download=False)\n",
    "        video_urls, video_ids = [], []\n",
    "\n",
    "        if 'entries' in result:\n",
    "            for entry in result['entries']:\n",
    "                if entry and 'id' in entry:\n",
    "                    video_ids.append(entry['id'])\n",
    "                    video_urls.append(f\"https://www.youtube.com/watch?v={entry['id']}\")\n",
    "                    print(\"url\", entry['id'], \"appended.\")\n",
    "\n",
    "    print(f\"‚úÖ Found {len(video_ids)} videos on channel: {CHANNEL_URL}\")\n",
    "    return video_ids, video_urls\n",
    "\n",
    "def crawler_youtube(video_ids, video_urls):\n",
    "    import yt_dlp\n",
    "    import whisper\n",
    "    from langchain_core.documents import Document\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    print(\"üé§ Transcribing YouTube videos ...\")\n",
    "    os.makedirs(\"YouTube\", exist_ok=True)\n",
    "    model = whisper.load_model(\"tiny\", device=\"cpu\")\n",
    "    youtube_docs = []\n",
    "\n",
    "    for video_url, video_id in zip(video_urls, video_ids):\n",
    "        print(f\"üì• Processing: {video_id}\")\n",
    "        output_file = f\"YouTube/{video_id}.mp3\"\n",
    "\n",
    "        # Download .mp3 if not already downloaded\n",
    "        if not Path(output_file).exists():\n",
    "            ret = os.system(f'yt-dlp -x --audio-format mp3 -o \"{output_file}\" {video_url}')\n",
    "            if ret != 0:\n",
    "                print(f\"‚ùå Failed to download: {video_url}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            result = model.transcribe(output_file)\n",
    "            print(f\"üìù Transcribed: {video_id}\")\n",
    "            doc = Document(\n",
    "                page_content=result[\"text\"],\n",
    "                metadata={\n",
    "                    \"source\": video_url,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"type\": \"YouTube\"\n",
    "                }\n",
    "            )\n",
    "            youtube_docs.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Transcription failed for {video_id}: {e}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Completed. Total documents created: {len(youtube_docs)}\")\n",
    "        print(\"Sample Doc:\", youtube_docs[:2])\n",
    "\n",
    "    return youtube_docs\n",
    "\n",
    "youtube_docs = crawler_youtube(fetch_video()[0], fetch_video()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler_web.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile crawler_web.py\n",
    "\n",
    "def crawl_internal_links(start_url, max_pages=10, max_depth=1):\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "    from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.common.exceptions import StaleElementReferenceException\n",
    "    import time\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Crawl internal URLs from a site using Selenium, with support for JavaScript-heavy pages.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL to start crawling from.\n",
    "        max_pages (int): Max number of pages to crawl.\n",
    "        max_depth (int): Max depth to crawl (0 = just root).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of internal URLs that were successfully visited.\n",
    "    \"\"\"\n",
    "    # Selenium Headless Browser Setup\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    visited = set()\n",
    "    domain = urlparse(start_url).netloc\n",
    "    to_visit = [(start_url, 0)]\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url, depth = to_visit.pop(0)\n",
    "        if url in visited or depth > max_depth:\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "            time.sleep(1)  # üîÅ Wait for JS to load\n",
    "\n",
    "            print(f\"Visited ({len(visited)+1}/{max_pages}), Depth {depth}): {url}\")\n",
    "            visited.add(url)\n",
    "\n",
    "            # If max depth reached, skip link extraction\n",
    "            if depth == max_depth:\n",
    "                continue\n",
    "\n",
    "            # Extract and queue internal links\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if not href or href.startswith((\"mailto:\", \"tel:\", \"javascript:\")):\n",
    "                        continue\n",
    "\n",
    "                    parsed = urlparse(href)\n",
    "                    if parsed.netloc == domain or parsed.netloc == \"\":\n",
    "                        full_url = urljoin(url, href).split(\"#\")[0]\n",
    "                        if full_url not in visited and all(full_url != q[0] for q in to_visit):\n",
    "                            to_visit.append((full_url, depth + 1))\n",
    "                except StaleElementReferenceException:\n",
    "                    continue\n",
    "\n",
    "        except (WebDriverException, TimeoutException):\n",
    "            print(f\"‚ö†Ô∏è Skipping (Error): {url}\")\n",
    "            visited.add(url)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n Total unique internal URLs visited: {len(visited)}\")\n",
    "\n",
    "    if len(visited) < max_pages:\n",
    "        print(\"‚ö†Ô∏è Number of crawled URLs is less than max_pages. Possible reasons:\")\n",
    "        print(\"- Site may not have enough unique pages within the allowed depth.\")\n",
    "        print(\"- Some links might be hidden behind JavaScript interactions.\")\n",
    "        print(\"- Some links could be blocked, inaccessible, or slow-loading.\")\n",
    "        print(\"- Your max_depth may be too shallow to discover deeper links, try changing depth.\")\n",
    "    \n",
    "    return list(visited)\n",
    "\n",
    "def change_web_to_meta_data(doc):\n",
    "    \"\"\"\n",
    "    Change the metadata of the web documents to include the content type.\n",
    "    \"\"\"\n",
    "    doc.metadata[\"type\"] = \"Web\"\n",
    "    # remove \\n from the content\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
    "    # remove multiple spaces\n",
    "    doc.page_content = ' '.join(doc.page_content.split())\n",
    "    return doc\n",
    "\n",
    "def crawler_web():\n",
    "    from langchain_community.document_loaders import WebBaseLoader\n",
    "    print(\"Web Crawler started ....\")\n",
    "\n",
    "    #### CONFIGURABLE SETTINGS \n",
    "    start_url = \"https://fidesinnova.io/\"\n",
    "    max_pages = 50   # Maximum number of pages to visit\n",
    "    max_depth = 3     # 0 = only root, 1 = root + links from root\n",
    "\n",
    "    ####  Run the Crawler\n",
    "    web_docs_list1 = crawl_internal_links(start_url, max_pages, max_depth)\n",
    "\n",
    "    #  Load the web documents\n",
    "    web_docs = []\n",
    "    for idx in web_docs_list1:\n",
    "        a = WebBaseLoader(idx)\n",
    "        print(f\"Loading {idx} ...\")\n",
    "        try:\n",
    "            temp_docs = a.load()\n",
    "            temp_docs = list(map(change_web_to_meta_data, temp_docs))\n",
    "\n",
    "            web_docs.extend(temp_docs)\n",
    "            print(idx + \" is loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"{idx} is not loaded. Error: {e}\")\n",
    "\n",
    "    print(f\"Total web documents loaded: {len(web_docs)}\")\n",
    "    print(\"Sample doc: \",web_docs[:2])  # Print first 2 documents for verification\n",
    "    return web_docs\n",
    "\n",
    "web_docs = crawler_web()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler_github.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler_github.py\n",
    "\n",
    "def change_GitHub_to_meta_data(doc):\n",
    "    \"\"\"\n",
    "    Change the metadata of the web documents to include the content type.\n",
    "    \"\"\"\n",
    "    doc.metadata[\"type\"] = \"GitHub\"\n",
    "    # remove \\n from the content\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
    "    # remove multiple spaces\n",
    "    doc.page_content = ' '.join(doc.page_content.split())\n",
    "    return doc\n",
    "\n",
    "def crawler_github():\n",
    "    from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "    # loading GitHub Repos\n",
    "    github_repos = [\n",
    "        \"https://github.com/TheArchitect2000/Fides-Innova-WiKi\",\n",
    "        \"https://github.com/TheArchitect2000/Blockchain-based-IoT-Server\",\n",
    "        \"https://github.com/TheArchitect2000/zk-IoT\",\n",
    "        \"https://github.com/TheArchitect2000/Smart-Contract-Protocol\",\n",
    "\n",
    "    #    \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-c\", \n",
    "    #    \"https://github.com/TheArchitect2000/ZKP-Blockchain-Explorer\",\n",
    "    #    \"https://github.com/TheArchitect2000/evm-server\",\n",
    "    #    \"https://github.com/TheArchitect2000/New-IoT-Device-Integration\",\n",
    "    #   \"https://github.com/TheArchitect2000/zkiot-riscv-qemu-rust\"\n",
    "    ]\n",
    "    \n",
    "    github_docs = []\n",
    "    for url in github_repos:\n",
    "        print(f\"üì• Loading repository: {url}\")\n",
    "        repo_name = url.split(\"/\")[-1]\n",
    "        local_path = f\"./cloned_repos/{repo_name}\"\n",
    "\n",
    "        loader = GitLoader(\n",
    "            repo_path=local_path,\n",
    "            clone_url=url,\n",
    "            branch=\"main\",\n",
    "            file_filter=lambda f: f.endswith((\n",
    "                # \".py\", \".md\", \".c\", \".cpp\", \".rs\", \".json\", \".html\",\n",
    "                # \".js\", \".ts\", \".css\", \".java\", \".txt\", \".yml\", \".yaml\", \".sh\"\n",
    "                \".md\"\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "        temp_docs = loader.load()\n",
    "        temp_docs = list(map(change_GitHub_to_meta_data, temp_docs))\n",
    "        github_docs.extend(temp_docs)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(github_docs)} documents from {repo_name}\")\n",
    "\n",
    "    print(f\"Total GitHub documents loaded: {len(github_docs)}\")\n",
    "    return github_docs\n",
    "\n",
    "github_docs = crawler_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler_pdf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler_pdf.py\n",
    "\n",
    "def change_pdf_doc(doc):\n",
    "    doc.metadata['type']='PDF'\n",
    "    return doc\n",
    "\n",
    "def crawler_pdf():\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    print(\"PDF Crawler started ....\")\n",
    "\n",
    "    pdf_docs = []\n",
    "    pdf_files = [\n",
    "        \"PDF/zkIoT.pdf\",\n",
    "        \"PDF/Consensus Algorithms.pdf\",\n",
    "        \"PDF/Data Monetization.pdf\",\n",
    "        \"pdf/Decentralized Delegated Proof.pdf\",\n",
    "        \"pdf/Digital Twins.pdf\",\n",
    "        \"pdf/Fides service contracts.pdf\",\n",
    "        \"pdf/fides_innova_gitbook_placeholder.pdf\",\n",
    "        \"pdf/IoT Startups.pdf\",\n",
    "        \"pdf/MIoTN.pdf\",\n",
    "        \"pdf/MQTT and MQTTS protocols.pdf\",\n",
    "        \"pdf/Service Contract.pdf\",\n",
    "        \"pdf/Service Market.pdf\",\n",
    "        \"pdf/What‚Äôs Web 3.0.pdf\"\n",
    "    ]\n",
    "\n",
    "    for path in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            pdf_docs.extend(loader.load())\n",
    "            print(len(pdf_docs))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PDF {path}: {e}\")\n",
    "\n",
    "    pdf_docs = list(map(change_pdf_doc, pdf_docs))\n",
    "\n",
    "    print(\"Loaded all PDF files:\", len(pdf_files))\n",
    "    print(\"Created docs:\", len(pdf_docs))\n",
    "    return pdf_docs\n",
    "\n",
    "pdf_docs = crawler_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af9baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler_pptx.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler_pptx.py\n",
    "\n",
    "# Add metadata\n",
    "def change_pptx_doc(doc):\n",
    "    doc.metadata['type'] = 'PPTX'\n",
    "    return doc\n",
    "\n",
    "def crawler_pptx():\n",
    "    from langchain_community.document_loaders import UnstructuredPowerPointLoader \n",
    "    print(\"PPTX Crawler started ....\")\n",
    "\n",
    "    pptx_docs = []\n",
    "    pptx_files = [\n",
    "        \"PPTX/FidesinnovaDeck-v11.pptx\"\n",
    "    ]\n",
    "\n",
    "    for path in pptx_files:\n",
    "        print(f\"üì• Loading PPTX: {path}\")\n",
    "        try:\n",
    "            loader = UnstructuredPowerPointLoader(path)\n",
    "            pptx_docs.extend(loader.load())\n",
    "            print(f\"‚úÖ Loaded: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading PPTX {path}: {e}\")\n",
    "\n",
    "    pptx_docs = list(map(change_pptx_doc, pptx_docs))\n",
    "\n",
    "    print(\"Loaded PPTX files:\", len(pptx_files))\n",
    "    print(\"Created docs:\", len(pptx_docs))\n",
    "    return pptx_docs\n",
    "\n",
    "pptx_docs = crawler_pptx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dae04f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combine_docs.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile combine_docs.py\n",
    "\n",
    "def combine_docs(web_docs, github_docs, youtube_docs, pdf_docs, pptx_docs):\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    print(\"Splitter started ....\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "    print(\"Splitting web_docs ...\")\n",
    "    split_web_docs = splitter.split_documents(web_docs or [])\n",
    "    print(\"Splitting github_docs ...\")\n",
    "    split_github_docs = splitter.split_documents(github_docs or [])\n",
    "    print(\"Splitting youtube_docs ...\")\n",
    "    split_youtube_docs = splitter.split_documents(youtube_docs or [])\n",
    "    print(\"Splitting pdf_docs ...\")\n",
    "    split_pdf_docs = splitter.split_documents(pdf_docs or [])\n",
    "    print(\"Splitting pptx_docs ...\")\n",
    "    split_pptx_docs = splitter.split_documents(pptx_docs or [])\n",
    "\n",
    "\n",
    "    # --- Step 8: Store in Vector DB ---\n",
    "    # Combine all split documents\n",
    "    all_split_docs = (\n",
    "        split_youtube_docs +\n",
    "        split_pdf_docs +\n",
    "        split_pptx_docs +\n",
    "        split_web_docs +\n",
    "        split_github_docs\n",
    "    )\n",
    "    print(\"Splitting done.\", len(all_split_docs), \" documents created.\")\n",
    "    return all_split_docs\n",
    "\n",
    "all_split_docs = combine_docs(web_docs, github_docs, youtube_docs, pdf_docs, pptx_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da46d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_faiss_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_faiss_index.py\n",
    "\n",
    "def create_faiss_index(all_split_docs):\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from langchain.vectorstores.faiss import FAISS\n",
    "    from langchain.docstore import InMemoryDocstore\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import faiss\n",
    "    import os\n",
    "\n",
    "    # ---------------------- Step 1: Embed All ----------------------\n",
    "    base_embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    texts_docs = [(doc.page_content, doc) for doc in all_split_docs if doc.page_content.strip()]\n",
    "    texts, docs = zip(*texts_docs)\n",
    "    raw_vectors = np.array(base_embedder.embed_documents(list(texts)))\n",
    "\n",
    "    # ---------------------- Step 2: Filter Invalid Vectors ----------------------\n",
    "    filtered_vectors = []\n",
    "    filtered_docs = []\n",
    "    for vec, doc in zip(raw_vectors, docs):\n",
    "        if np.all(np.isfinite(vec)) and np.linalg.norm(vec) > 1e-6:\n",
    "            filtered_vectors.append(vec)\n",
    "            filtered_docs.append(doc)\n",
    "\n",
    "    raw_vectors = np.array(filtered_vectors)\n",
    "\n",
    "    # ---------------------- Step 3: Clip Outliers ----------------------\n",
    "    raw_vectors = np.clip(raw_vectors, -1000, 1000)\n",
    "    # ---------------------- Step 4: Normalize ----------------------\n",
    "    scaler = StandardScaler()\n",
    "    normalized_vectors = scaler.fit_transform(raw_vectors)\n",
    "\n",
    "    # ---------------------- Step 4.5: Drop Low-Variance Features ----------------------\n",
    "    variances = np.var(normalized_vectors, axis=0)\n",
    "    stable_mask = variances > 1e-6\n",
    "    normalized_vectors = normalized_vectors[:, stable_mask]\n",
    "    print(f\"‚úÖ Retained {np.sum(stable_mask)} stable features out of {len(stable_mask)}.\")\n",
    "\n",
    "    # ---------------------- Step 5: PCA Fit ----------------------\n",
    "    pca = PCA(n_components=256, svd_solver='full')\n",
    "    pca.fit(normalized_vectors)\n",
    "\n",
    "    # ---------------------- Step 6: Final Check Before PCA Transform ----------------------\n",
    "    clean_rows = []\n",
    "    clean_docs = []\n",
    "\n",
    "    for vec, doc in zip(normalized_vectors, filtered_docs):\n",
    "        if np.all(np.isfinite(vec)) and np.linalg.norm(vec) < 100:  # tighter threshold\n",
    "            clean_rows.append(vec)\n",
    "            clean_docs.append(doc)\n",
    "\n",
    "    normalized_vectors = np.array(clean_rows)\n",
    "    filtered_docs = clean_docs\n",
    "\n",
    "    # Now safely transform\n",
    "    transformed_vectors = pca.transform(normalized_vectors)\n",
    "\n",
    "    print(\"PCA transformation completed successfully.\")\n",
    "    print(\"Transformed vectors shape:\", transformed_vectors.shape)\n",
    "    print(\"Any NaN in transformed vectors?\", np.isnan(transformed_vectors).any())\n",
    "    print(\"Max component magnitude:\", np.max(np.abs(transformed_vectors)))\n",
    "    print(\"PCA components shape:\", pca.components_.shape)\n",
    "    print(\"Any NaN in PCA components?\", np.isnan(pca.components_).any())\n",
    "    print(\"Max component magnitude:\", np.max(np.abs(pca.components_)))\n",
    "\n",
    "\n",
    "    # ---------------------- Step 7: Create FAISS Index ----------------------\n",
    "    index = faiss.IndexFlatL2(transformed_vectors.shape[1])\n",
    "    index.add(transformed_vectors.astype(\"float32\"))\n",
    "\n",
    "    docstore = InMemoryDocstore(dict(enumerate(filtered_docs)))  # <-- use aligned docs\n",
    "    index_to_docstore_id = {i: i for i in range(len(filtered_docs))}\n",
    "\n",
    "    faiss_index = FAISS(\n",
    "        embedding_function=None,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id\n",
    "    )\n",
    "\n",
    "    # ---------------------- Step 8: Save Everything ----------------------\n",
    "    saved_dir = \"fides_faiss_crawled_data\"\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "\n",
    "    faiss_index.save_local(os.path.join(saved_dir, \"fides_faiss_pca_256\"))\n",
    "\n",
    "    with open(os.path.join(saved_dir, \"fides_pca_256_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "    with open(os.path.join(save_dir, \"fides_scaler.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    with open(os.path.join(save_dir, \"fides_feature_mask.npy\"), \"wb\") as f:\n",
    "        np.save(f, stable_mask)\n",
    "\n",
    "    print(\"‚úÖ All components saved: FAISS index, PCA model, Scaler, and feature mask.\")\n",
    "    return\n",
    "\n",
    "create_faiss_index(all_split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6db828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile search.py\n",
    "\n",
    "def load_faiss_components():\n",
    "    from langchain.vectorstores.faiss import FAISS\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    # ---------------------- Step 1: Load Components ----------------------\n",
    "    save_dir = \"fides_faiss_crawled_data\"\n",
    "\n",
    "    # Load FAISS index\n",
    "    faiss_index = FAISS.load_local(\n",
    "        folder_path=os.path.join(save_dir, \"fides_faiss_pca_256\"),\n",
    "        embeddings=None,\n",
    "        index_name=\"index\",  # default inside LangChain\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    # Load PCA\n",
    "    with open(os.path.join(save_dir, \"fides_pca_256_model.pkl\"), \"rb\") as f:\n",
    "        pca = pickle.load(f)\n",
    "\n",
    "    # Load scaler\n",
    "    with open(os.path.join(save_dir, \"fides_scaler.pkl\"), \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # Load feature mask\n",
    "    stable_mask = np.load(os.path.join(save_dir, \"fides_feature_mask.npy\"))\n",
    "    return\n",
    "\n",
    "# ---------------------- Step 2: Embed and Preprocess Query ----------------------\n",
    "def preprocess_query(text: str) -> np.ndarray:\n",
    "    embedder = base_embedder\n",
    "    vector = embedder.embed_query(text)\n",
    "\n",
    "    # Ensure vector is safe\n",
    "    if not np.all(np.isfinite(vector)) or np.linalg.norm(vector) <= 1e-6:\n",
    "        raise ValueError(\"Invalid or zero vector produced by embedder.\")\n",
    "\n",
    "    # Clip + normalize + select features\n",
    "    vector = np.clip(vector, -1000, 1000)\n",
    "    vector = scaler.transform([vector])\n",
    "    vector = vector[:, stable_mask]\n",
    "    vector = pca.transform(vector)\n",
    "\n",
    "    return vector.astype(\"float32\")\n",
    "\n",
    "# ---------------------- Step 3: Perform Similarity Search ----------------------\n",
    "def search(query: str, k=3):\n",
    "    query_vector = preprocess_query(query)\n",
    "    load_faiss_components()\n",
    "    return faiss_index.similarity_search_by_vector(query_vector[0], k=k)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
