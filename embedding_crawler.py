# -*- coding: utf-8 -*-
"""embedding-crawler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1veoXGbqXGOxHJrYH6ZehSCyoLBhXvML9
"""

# for embedding.py file
# python3 -m venv venv
# source venv/bin/activate
# pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 SpeechRecognition opencv-python
# python3 embedding.py
#%pip install pyaudio -q

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade pip -q
# %pip install python-dotenv langchain_openai langchain_community chromadb youtube-transcript-api pytube pypdf web3 -q
# %pip install youtube-transcript-api bs4 pypdf -q
# %pip install SpeechRecognition -q
# %pip install opencv-python -q
# %pip install beautifulsoup4 -q

# --- Step 1: 
from dotenv import load_dotenv
import os
import time
import requests
import yt_dlp
from bs4 import BeautifulSoup

from langchain_community.document_loaders import PyPDFLoader, YoutubeLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


# --- Step 2: Load environment variables from .env file
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# --- Step 3: Web crawler
headers = {
    "User-Agent": "Mozilla/5.0 (compatible; FidesCrawler/1.0)"
}

web_urls = [
    "https://www.fidesinnova.io/",
    "https://fidesinnova.io/devices/",
    "https://fidesinnova.io/Contacts/",
    "https://fidesinnova.io/courses/",
    "https://linktr.ee/fidesinnova/",
    "https://explorer.fidesinnova.io/",
    "https://play.google.com/store/apps/details?id=io.fidesinnova.front&pli=1/",
    "https://apps.apple.com/ca/app/fidesinnova/id6477492757/",
    "https://fidesinnova.io/blog-standard/",
    "https://fidesinnova.io/About/",
    "https://fidesinnova.io/#articles/",
    "https://discord.com/invite/NQdM6JGwcs/",
    "https://www.youtube.com/@FidesInnova/playlists/",
    "https://x.com/FidesInnova/",
    "https://fidesinnova.io/service-market/",
    "https://fidesinnova.io/service-contract-2/",
    "https://fidesinnova.io/d2pos/",
    "https://fidesinnova.io/web3/",
    "https://fidesinnova.io/service-contract/",
    "https://fidesinnova.io/miotn/",
    "https://fidesinnova.io/consensus-algorithms/",
    "https://fidesinnova.io/mqtt-mqtts/"
]

def crawl_web_url(url):
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string.strip() if soup.title else "No Title"
        text = soup.get_text(separator='\n', strip=True)
        return Document(page_content=text, metadata={"source": url, "title": title, "type": "Web"})
    except Exception as e:
        print(f"Error crawling {url}: {e}")
        return None

web_docs = [doc for url in web_urls if (doc := crawl_web_url(url))]
print("=== ALL web_docs ===)")
print(len(web_docs))

# --- Step 4: GitHub Crawler ---
github_urls = [
    "https://github.com/TheArchitect2000/iot-server/",
    "https://github.com/TheArchitect2000/zkiot-riscv-qemu-c/",
    "https://github.com/TheArchitect2000/Fides-Innova-WiKi/",
    "https://github.com/TheArchitect2000/zkiot-arm-siemens-iot2050-c/",
    "https://github.com/TheArchitect2000/fidesinnova-smart-contract-protocol/",
    "https://github.com/TheArchitect2000/zkp-explorer/",
    "https://github.com/TheArchitect2000/Fides-LLM/",
    "https://github.com/TheArchitect2000/Verifiable-Computing-AI-Agents/",
    "https://github.com/TheArchitect2000/evm-server/",
    "https://github.com/TheArchitect2000/New-IoT-Device-Integration/",
    "https://github.com/TheArchitect2000/zkiot-riscv-qemu-rust/"
]

def crawl_github_url(url):
    
    headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                    "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string.strip() if soup.title else "No Title"
        text = soup.get_text(separator='\n', strip=True)
        return Document(page_content=text, metadata={"source": url, "title": title, "type": "Web"})
    except Exception as e:
        print(f"Error crawling GitHub {url}: {e}")
        return None

github_docs = [doc for url in github_urls if (doc := crawl_github_url(url))]

print("=== ALL github_docs ===)")
print(len(github_docs))

# --- Step 5: YouTube Channel Crawler ---

# Fides Innova YouTube Channel ID
CHANNEL_ID = "UCrrqGYx98H1dPdZsNb1i9-g"
CHANNEL_URL = f"https://www.youtube.com/channel/{CHANNEL_ID}"

def fetch_video_urls(channel_url: str):
    ydl_opts = {
        'ignoreerrors': True,
        'quiet': True,
        'extract_flat': True,  # Only metadata, not downloading
        'force_generic_extractor': False,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        result = ydl.extract_info(channel_url, download=False)
        video_urls = []
        video_ids = []

        if 'entries' in result:
            for entry in result['entries']:
                if entry and 'id' in entry:
                    video_url = f"https://www.youtube.com/watch?v={entry['id']}"
                    video_urls.append(video_url)
                    video_ids.append(entry['id'])
        return video_ids

# Fetch and display video URLs
video_ids = fetch_video_urls(CHANNEL_URL)
print(f"✅ Found {len(video_ids)} videos on channel.\n")

youtube_docs = []
for idx in video_ids:
    a = YoutubeLoader(idx)
    try:
        youtube_docs.extend(a.load())
        print(idx)
    except:
        print(f"{idx} not loaded.")
        try:
            time.sleep(8)
            youtube_docs.extend(a.load())
            print(idx)
        except:
            print(f"{idx} not loaded.")
            pass

def change_YouTube_doc(doc):
    doc.metadata['type']='YouTube'
    return doc

youtube_docs = list(map(change_YouTube_doc, youtube_docs))

print("=== ALL youtube_docs ===)")
print(len(youtube_docs))

# --- Step 6: Load PDFs ---
pdf_docs = []
pdf_files = [
    "PDF/zkIoT.pdf",
    "PDF/Fides Innova Company Profile 2025.pdf",
    "PDF/Fides Innova Pitch Deck - v10.pdf"
]

for path in pdf_files:
    try:
        loader = PyPDFLoader(path)
        pdf_docs.extend(loader.load())
    except Exception as e:
        print(f"Error loading PDF {path}: {e}")

def change_pdf_doc(doc):
    doc.metadata['type']='PDF'
    return doc

pdf_docs = list(map(change_pdf_doc, pdf_docs))

print("=== ALL PDFs ===)")
print(len(pdf_docs))

# --- Step 7: Split & Vectorize ---
all_docs = web_docs + github_docs + youtube_docs + pdf_docs

splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)
split_docs = splitter.split_documents(all_docs)

embedding = OpenAIEmbeddings(model="text-embedding-3-large")
vectordb = Chroma(
    collection_name="fides_crawled_data",
    embedding_function=embedding,
    persist_directory="./chroma_langchain_db"
)

vectordb.add_documents(split_docs)
print("✅ All documents crawled, split, and stored in vector DB.")

print("=== ALL Doc.s ===)")
print(len(all_docs))